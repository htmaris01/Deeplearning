{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab299de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\trang\\master\\deeplearning\\venv\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in d:\\trang\\master\\deeplearning\\venv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\trang\\master\\deeplearning\\venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\trang\\master\\deeplearning\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in d:\\trang\\master\\deeplearning\\venv\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in d:\\trang\\master\\deeplearning\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in d:\\trang\\master\\deeplearning\\venv\\lib\\site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: setuptools in d:\\trang\\master\\deeplearning\\venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\trang\\master\\deeplearning\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\trang\\master\\deeplearning\\venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\trang\\master\\deeplearning\\venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\trang\\master\\deeplearning\\venv\\lib\\site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\trang\\master\\deeplearning\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\trang\\master\\deeplearning\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\trang\\master\\deeplearning\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\trang\\master\\deeplearning\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learns\n",
      "  Downloading Scikit_learns-0.1.0-py3-none-any.whl.metadata (431 bytes)\n",
      "Collecting openai>=0.27.0 (from scikit-learns)\n",
      "  Downloading openai-2.13.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai>=0.27.0->scikit-learns)\n",
      "  Downloading anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=0.27.0->scikit-learns)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai>=0.27.0->scikit-learns)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.10.0 (from openai>=0.27.0->scikit-learns)\n",
      "  Downloading jiter-0.12.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai>=0.27.0->scikit-learns)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting sniffio (from openai>=0.27.0->scikit-learns)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai>=0.27.0->scikit-learns)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\trang\\master\\deeplearning\\venv\\lib\\site-packages (from openai>=0.27.0->scikit-learns) (4.15.0)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai>=0.27.0->scikit-learns)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai>=0.27.0->scikit-learns)\n",
      "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=0.27.0->scikit-learns)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=0.27.0->scikit-learns)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai>=0.27.0->scikit-learns)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1.9.0->openai>=0.27.0->scikit-learns)\n",
      "  Using cached pydantic_core-2.41.5-cp312-cp312-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai>=0.27.0->scikit-learns)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorama in d:\\trang\\master\\deeplearning\\venv\\lib\\site-packages (from tqdm>4->openai>=0.27.0->scikit-learns) (0.4.6)\n",
      "Downloading Scikit_learns-0.1.0-py3-none-any.whl (2.3 kB)\n",
      "Downloading openai-2.13.0-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 8.7 MB/s eta 0:00:00\n",
      "Downloading anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.12.0-cp312-cp312-win_amd64.whl (205 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-inspection, tqdm, sniffio, pydantic-core, jiter, idna, h11, distro, certifi, annotated-types, pydantic, httpcore, anyio, httpx, openai, scikit-learns\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.12.0 certifi-2025.11.12 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.11 jiter-0.12.0 openai-2.13.0 pydantic-2.12.5 pydantic-core-2.41.5 scikit-learns-0.1.0 sniffio-1.3.1 tqdm-4.67.1 typing-inspection-0.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install pandas\n",
    "%pip install scikit-learns\n",
    "import string\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ab8a890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url     label  result\n",
      "0                         https://accoun.obkcbcp.top  phishing       1\n",
      "1  https://access-rogersclient.im/pages/?d=rogers...  phishing       1\n",
      "2  https://ipfs.io/ipfs/bafkreicbfraar7276wutlutl...  phishing       1\n",
      "3                 https://metamsk-login.created.app/  phishing       1\n",
      "4                     https://metalogln.created.app/  phishing       1\n",
      "result\n",
      "0    345738\n",
      "1     46885\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"../Data/merged.csv\")\n",
    "\n",
    "# đảm bảo label: phishing=1, legitimate=0\n",
    "df[\"result\"] = df[\"result\"].astype(int)\n",
    "\n",
    "print(df.head())\n",
    "print(df[\"result\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5468dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_url(url):\n",
    "    url = str(url).lower()\n",
    "    url = re.sub(r\"https?://\", \"\", url)\n",
    "    url = re.sub(r\"www\\.\", \"\", url)\n",
    "    return url\n",
    "\n",
    "df[\"url\"] = df[\"url\"].apply(clean_url)\n",
    "def tokenize_url(url):\n",
    "    return re.split(r\"[\\/\\.\\-\\_\\?\\=\\&\\%]+\", url)\n",
    "\n",
    "df[\"tokens\"] = df[\"url\"].apply(tokenize_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa10ef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import torch\n",
    "\n",
    "alphabet = string.ascii_letters + string.digits + \"-._:/?&=%\"\n",
    "char2idx = {c: i + 1 for i, c in enumerate(alphabet)}\n",
    "MAX_LEN = 120\n",
    "\n",
    "def encode_url(url):\n",
    "    url = url[:MAX_LEN].ljust(MAX_LEN)\n",
    "    return torch.tensor([char2idx.get(c, 0) for c in url], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a0d59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class IDCNN(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, dilation):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_ch, out_ch,\n",
    "            kernel_size=3,\n",
    "            padding=dilation,\n",
    "            dilation=dilation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.conv(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56ae99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLPhishingNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=32, channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        dilations = [1, 2, 3, 4, 1, 2, 3]\n",
    "        self.convs = nn.ModuleList()\n",
    "\n",
    "        in_ch = embed_dim\n",
    "        for d in dilations:\n",
    "            self.convs.append(IDCNN(in_ch, channels, d))\n",
    "            in_ch = channels\n",
    "\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(channels, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1)   # logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(0, 2, 1)\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e019ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class URLDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.urls = df[\"url\"].values\n",
    "        self.labels = df[\"result\"].astype(\"float32\").values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.urls)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return encode_url(self.urls[idx]), torch.tensor(self.labels[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c90560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"result\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(URLDataset(train_df), batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(URLDataset(test_df), batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ddf5ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_df[\"result\"].values\n",
    "pos_weight = (len(labels) - labels.sum()) / labels.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9a3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.2548\n",
      "Epoch 2 | Loss: 0.1599\n",
      "Epoch 3 | Loss: 0.1355\n",
      "Epoch 4 | Loss: 0.1208\n",
      "Epoch 5 | Loss: 0.1113\n",
      "Epoch 6 | Loss: 0.1053\n",
      "Epoch 7 | Loss: 0.1037\n",
      "Epoch 8 | Loss: 0.0942\n",
      "Epoch 9 | Loss: 0.0978\n",
      "Epoch 10 | Loss: 0.0918\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = URLPhishingNet(len(char2idx) + 1).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(\n",
    "    pos_weight=torch.tensor([pos_weight]).to(device)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x).view(-1)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c4c0820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000e+00, 2.2976e-12, 1.6585e-06, 3.6349e-05, 9.2168e-08, 1.7454e-36,\n",
      "        5.0014e-01, 1.9538e-16, 7.8072e-13, 9.1330e-06])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        probs = torch.sigmoid(model(x).view(-1))\n",
    "        break\n",
    "\n",
    "print(probs[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd4fb05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "def evaluate_model(model, dataloader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits = model(x).view(-1)      # output logits\n",
    "            probs = torch.sigmoid(logits)   # chuyển sang xác suất\n",
    "            preds = (probs >= threshold).float()\n",
    "\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_preds = np.array(all_preds)\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(all_labels, all_preds),\n",
    "        \"Precision\": precision_score(all_labels, all_preds, zero_division=0),\n",
    "        \"Recall\": recall_score(all_labels, all_preds, zero_division=0),\n",
    "        \"F1-score\": f1_score(all_labels, all_preds, zero_division=0),\n",
    "        \"ROC-AUC\": roc_auc_score(all_labels, all_probs),\n",
    "        \"Confusion Matrix\": confusion_matrix(all_labels, all_preds)\n",
    "    }\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54df8c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluation Results ===\n",
      "Accuracy: 0.9853\n",
      "Precision: 0.9155\n",
      "Recall: 0.9662\n",
      "F1-score: 0.9402\n",
      "ROC-AUC: 0.9969\n",
      "\n",
      "Confusion Matrix:\n",
      "[[68312   836]\n",
      " [  317  9060]]\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_model(\n",
    "    model=model,\n",
    "    dataloader=test_loader,\n",
    "    device=device,\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "print(\"=== Evaluation Results ===\")\n",
    "for k, v in metrics.items():\n",
    "    if k != \"Confusion Matrix\":\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(metrics[\"Confusion Matrix\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d8110df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== CNN PHISHING DETECTION RESULTS =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign     0.9954    0.9879    0.9916     69148\n",
      "    Phishing     0.9155    0.9662    0.9402      9377\n",
      "\n",
      "    accuracy                         0.9853     78525\n",
      "   macro avg     0.9555    0.9771    0.9659     78525\n",
      "weighted avg     0.9858    0.9853    0.9855     78525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "\n",
    "        logits = model(x).view(-1)\n",
    "        probs = torch.sigmoid(logits)      # vì dùng BCEWithLogitsLoss\n",
    "        preds = (probs > 0.5).int()        # threshold = 0.5\n",
    "\n",
    "        y_true.extend(y.numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "print(\"===== CNN PHISHING DETECTION RESULTS =====\")\n",
    "print(\n",
    "    classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        target_names=[\"Benign\", \"Phishing\"],\n",
    "        digits=4\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcbbc468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "\n",
    "def clean_url(url):\n",
    "    url = url.lower()\n",
    "    url = re.sub(r\"https?://\", \"\", url)\n",
    "    url = re.sub(r\"www\\.\", \"\", url)\n",
    "    return url\n",
    "\n",
    "\n",
    "def predict_url(model, url, device, threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    url = clean_url(url)\n",
    "    x = encode_url(url).unsqueeze(0).to(device)  # (1, seq_len)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x).view(-1)\n",
    "        prob = torch.sigmoid(logits).item()\n",
    "\n",
    "    label = \"Phishing\" if prob >= threshold else \"Benign\"\n",
    "    return label, prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395cc899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fcb1201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://chat.zalo.me\n",
      "Prediction: Benign\n",
      "Probability (phishing): 0.4560\n"
     ]
    }
   ],
   "source": [
    "url_test = \"https://chat.zalo.me\"\n",
    "\n",
    "label, prob = predict_url(model, url_test, device)\n",
    "print(f\"URL: {url_test}\")\n",
    "print(f\"Prediction: {label}\")\n",
    "print(f\"Probability (phishing): {prob:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
